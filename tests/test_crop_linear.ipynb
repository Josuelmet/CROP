{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Download CROP if necessary\n",
        "\n",
        "import os\n",
        "import sys\n",
        "if 'crop.py' not in os.listdir('..'):\n",
        "    !git clone https://github.com/Josuelmet/CROP.git\n",
        "    sys.path.append('./CROP')\n",
        "else:\n",
        "    sys.path.append('..')\n",
        "\n",
        "from crop import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbCKbRXoqo9U",
        "outputId": "c759176d-4b66-4c98-f5e2-6cb1beb67f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CROP'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 56 (delta 16), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (56/56), 654.37 KiB | 3.41 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing CROP on a random high-dimensional model"
      ],
      "metadata": {
        "id": "b5pZF0x990At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "model =   nn.Sequential(\n",
        "          OrderedDict([\n",
        "          (\"flat1\", nn.Flatten()),\n",
        "          (\"fc0\", nn.Linear(32*32*1, 400)),\n",
        "          # Affine Layers\n",
        "          (\"fc1\", nn.Linear(400, 120)),\n",
        "          (\"relu3\", nn.LeakyReLU()),\n",
        "          (\"fc2\", nn.Linear(120, 84)),\n",
        "          (\"relu4\", nn.LeakyReLU()),\n",
        "          (\"fc3\", nn.Linear(84, 10))\n",
        "          ]))\n",
        "\n",
        "model = ConstrainedSequential.cast(model)\n",
        "\n",
        "b = 100\n",
        "c = torch.randn(4, b, 1, 32, 32)\n",
        "x = torch.randn(1, 1, 32, 32)\n",
        "\n",
        "model(x, c)\n",
        "model = ConstrainedSequential.uncast(model)"
      ],
      "metadata": {
        "id": "SH54spGf92c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for each_constraint in c:\n",
        "    flags, unmatched_act = check_layerwise_signs(model, each_constraint)\n",
        "\n",
        "    # Index up to :-1 because the last layer's signs do not need to agree.\n",
        "    print('All signs agree', all(flags[:-1]))\n",
        "    print('Abs sum of activations that disagree',sum(unmatched_act[:-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U-2L3lyMV4m",
        "outputId": "52fec0e0-c4b7-4ab6-d0bd-224d9d05f5a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All signs agree True\n",
            "Abs sum of activations that disagree tensor(0.)\n",
            "All signs agree True\n",
            "Abs sum of activations that disagree tensor(0.)\n",
            "All signs agree True\n",
            "Abs sum of activations that disagree tensor(0.)\n",
            "All signs agree True\n",
            "Abs sum of activations that disagree tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Line-by-line testing"
      ],
      "metadata": {
        "id": "eRgHQUiZHqnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "m = nn.Linear(10, 3)\n",
        "x = torch.randn(8, 10)\n",
        "R, V = (2, 4)\n",
        "assert R * V == x.shape[0]\n",
        "\n",
        "force_linearity = False\n",
        "\n",
        "h = m(x)\n",
        "\n",
        "def sign(tensor):\n",
        "    return tensor.sign() + (tensor == 0)\n",
        "\n",
        "#@torch.no_grad()\n",
        "\n",
        "# Given h, the pre-activation for everyone (data + constraints).\n",
        "# Shape is thus (N + R * V, K) with K the output dim (since W is R^D:-> R^K)\n",
        "h_c = torch.clone(h[-R * V:])\n",
        "h_c_signs = sign(h_c).reshape((R, V) + h_c.shape[1:])"
      ],
      "metadata": {
        "id": "NM4rZVx4HSCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_c_signs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca3qIxe43-4o",
        "outputId": "fced9cb9-3288-4081-980d-9255542e2790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.,  1.,  1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1.,  1.,  1.],\n",
              "         [ 1., -1., -1.]],\n",
              "\n",
              "        [[ 1.,  1., -1.],\n",
              "         [-1., -1., -1.],\n",
              "         [-1.,  1.,  1.],\n",
              "         [ 1., -1.,  1.]]], grad_fn=<ReshapeAliasBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select which units/neurons actually need intervention;\n",
        "# i.e., which neurons do not have signs that agree within each of the R constraint regions.\n",
        "# conflict_dims is a length-K boolean vector.\n",
        "conflict_dims = (h_c_signs.sum(1).abs() != V).any(0)\n",
        "conflict_dims"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUnr7qH08Sf9",
        "outputId": "26234c2e-7431-457a-9e22-91609cd1dadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the overall majority sign:\n",
        "desired_signs = sign(h_c_signs.sum((0,1)))[conflict_dims]\n",
        "desired_signs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DQfnqDL9BDj",
        "outputId": "83130f38-3621-4712-ee79-4bada04a4ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.,  1.,  1.], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate each region's majority sign for each neuron.\n",
        "# regionwise_majority has shape (R, K_conflict)\n",
        "regionwise_majority = sign(h_c_signs.sum(1))[:, conflict_dims]\n",
        "regionwise_majority"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqCNSBh77eqn",
        "outputId": "d48cd709-7795-4193-a190-98dfb80f3491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.,  1.,  1.],\n",
              "        [ 1.,  1.,  1.]], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not force_linearity:\n",
        "\n",
        "    # Reshape the conflicted part of h_c to (R, V, K_conflict), then\n",
        "    # multiply by 0 all neurons that do not agree with the regionwise majority.\n",
        "    h_c[:, conflict_dims] = (\n",
        "        h_c.reshape_as(h_c_signs)[:, :, conflict_dims] * (regionwise_majority == desired_signs).unsqueeze(1)\n",
        "    ).reshape_as(h_c[:, conflict_dims])\n",
        "\n",
        "    print(h_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NveBaI9uWel",
        "outputId": "6b185175-d8a3-4e33-8c93-737ca04230b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2121,  0.3760,  0.6179],\n",
            "        [-0.7720, -0.2179, -0.7318],\n",
            "        [-1.3371,  0.9049,  0.5732],\n",
            "        [ 1.3041, -0.1099, -1.1811],\n",
            "        [ 0.0000,  0.0483, -0.6730],\n",
            "        [-0.0000, -0.6817, -0.3817],\n",
            "        [-0.0000,  0.0661,  1.1537],\n",
            "        [ 0.0000, -0.4754,  0.3144]], grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look by how much do we have to shift each hyper-plane\n",
        "# so that all constraints have the majority sign\n",
        "extra_bias = (h_c[:, conflict_dims] * desired_signs).amin(0).clamp(max=0) * desired_signs * (1 + 1e-3)\n",
        "h[:, conflict_dims] -= extra_bias\n",
        "\n",
        "h.sign()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEjF63XKvbRy",
        "outputId": "0e297a83-e6f1-4b92-ee11-404dd378b646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.,  1.,  1.],\n",
              "        [-1.,  1.,  1.],\n",
              "        [-1.,  1.,  1.],\n",
              "        [-1.,  1.,  1.],\n",
              "        [-1.,  1.,  1.],\n",
              "        [-1.,  1.,  1.],\n",
              "        [-1.,  1.,  1.],\n",
              "        [-1.,  1.,  1.]], grad_fn=<SignBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}